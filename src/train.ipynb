{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nbimporter\n",
      "  Downloading nbimporter-0.3.4-py3-none-any.whl.metadata (252 bytes)\n",
      "Downloading nbimporter-0.3.4-py3-none-any.whl (4.9 kB)\n",
      "Installing collected packages: nbimporter\n",
      "Successfully installed nbimporter-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nbimporter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Encoder output shape: (None, 512)\n",
      "Decoder output shape: (None, 35, 512)\n",
      "Expanded image features shape: (None, 35, 512)\n",
      "Loading data...\n",
      "Starting training...\n",
      "Epoch 1/5\n",
      "\u001b[1m1495/2686\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:18:15\u001b[0m 4s/step - accuracy: 0.7635 - loss: 1.5552"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of 'src' to the Python path\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "src_path = os.path.abspath(os.path.join(current_dir, \"src\"))  # Adjust path to 'src'\n",
    "sys.path.append(src_path)\n",
    "\n",
    "# Import the model definition (converted to a Python script)\n",
    "from model import build_image_captioning_model\n",
    "\n",
    "# Paths to processed data\n",
    "image_features_path = \"/Users/aryan/Desktop/Image-Caption-Generator/data/image_features.npy\"\n",
    "captions_path = \"/Users/aryan/Desktop/Image-Caption-Generator/data/processed_captions.npy\"\n",
    "tokenizer_path = \"/Users/aryan/Desktop/Image-Caption-Generator/data/tokenizer.pkl\"\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Load data\n",
    "def load_data():\n",
    "    image_features = np.load(image_features_path, allow_pickle=True).item()\n",
    "    captions = np.load(captions_path, allow_pickle=True).item()\n",
    "    return image_features, captions\n",
    "\n",
    "# Data generator\n",
    "def data_generator(image_features, captions, tokenizer, max_caption_length, batch_size):\n",
    "    while True:\n",
    "        image_inputs, caption_inputs, targets = [], [], []\n",
    "        for image_id, caption_set in captions.items():\n",
    "            for caption in caption_set:\n",
    "                input_seq = caption[:-1]  # Exclude the last word for inputs\n",
    "                target_seq = caption[1:]  # Shift the sequence by one for targets\n",
    "\n",
    "                input_seq_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                    [input_seq], maxlen=max_caption_length, padding=\"post\"\n",
    "                )[0]\n",
    "                target_seq_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                    [target_seq], maxlen=max_caption_length, padding=\"post\"\n",
    "                )[0]\n",
    "\n",
    "                image_inputs.append(image_features[image_id])\n",
    "                caption_inputs.append(input_seq_padded)\n",
    "                targets.append(target_seq_padded)\n",
    "\n",
    "                if len(image_inputs) == batch_size:\n",
    "                    yield (\n",
    "                        (np.array(image_inputs), np.array(caption_inputs)),\n",
    "                        np.array(targets),\n",
    "                    )\n",
    "                    image_inputs, caption_inputs, targets = [], [], []\n",
    "\n",
    "print(\"Building model...\")\n",
    "model = build_image_captioning_model()\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "with open(tokenizer_path, \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Include padding token\n",
    "max_caption_length = 35  # Match preprocessing\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "image_features, captions = load_data()\n",
    "\n",
    "# Create the dataset\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(image_features, captions, tokenizer, max_caption_length, BATCH_SIZE),\n",
    "    output_signature=(\n",
    "        (\n",
    "            tf.TensorSpec(shape=(BATCH_SIZE, 2048), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(BATCH_SIZE, max_caption_length), dtype=tf.int32),\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(BATCH_SIZE, max_caption_length), dtype=tf.int32),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "steps_per_epoch = sum(len(captions[image]) * (len(captions[image][0]) - 1) for image in captions) // BATCH_SIZE\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"model_checkpoint.keras\", save_best_only=True, monitor=\"loss\", mode=\"min\"\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor=\"loss\", patience=3, mode=\"min\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "model.fit(\n",
    "    dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint, early_stopping],\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
