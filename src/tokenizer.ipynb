{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading captions...\n",
      "Cleaning captions...\n",
      "Tokenizing captions...\n",
      "Vocabulary size: 8780\n",
      "Maximum caption length: 35\n",
      "Encoding and padding captions...\n",
      "Saving tokenizer and processed captions...\n",
      "Tokenizer saved to /Users/aryan/Desktop/Image-Caption-Generator/data/tokenizer.pkl\n",
      "Processed captions saved to /Users/aryan/Desktop/Image-Caption-Generator/data/processed_captions.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Paths\n",
    "captions_file = \"/Users/aryan/Desktop/Image-Caption-Generator/data/captions.txt\"\n",
    "processed_captions_file = \"/Users/aryan/Desktop/Image-Caption-Generator/data/processed_captions.npy\"\n",
    "tokenizer_file = \"/Users/aryan/Desktop/Image-Caption-Generator/data/tokenizer.pkl\"\n",
    "\n",
    "# Load captions\n",
    "def load_captions(file_path):\n",
    "    captions = {}\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                image, caption = line.strip().split(\",\", 1)  # Split image and caption\n",
    "                if image == \"image\" or caption == \"caption\":  # Skip header or invalid lines\n",
    "                    continue\n",
    "                if image not in captions:\n",
    "                    captions[image] = []\n",
    "                captions[image].append(caption)\n",
    "            except ValueError:\n",
    "                print(f\"Skipping malformed line: {line.strip()}\")\n",
    "    return captions\n",
    "\n",
    "# Clean captions\n",
    "def clean_caption(caption):\n",
    "    caption = caption.lower()\n",
    "    caption = re.sub(r\"[^a-z ]\", \"\", caption)  # Remove special characters and numbers\n",
    "    caption = re.sub(r\"\\s+\", \" \", caption).strip()  # Remove extra spaces\n",
    "    return caption\n",
    "\n",
    "# Preprocess all captions\n",
    "def preprocess_captions(captions):\n",
    "    cleaned_captions = {}\n",
    "    for image, caps in captions.items():\n",
    "        cleaned_captions[image] = [clean_caption(cap) for cap in caps]\n",
    "    return cleaned_captions\n",
    "\n",
    "# Tokenize captions\n",
    "def tokenize_captions(captions, max_vocab_size=10000):\n",
    "    all_captions = [cap for caps in captions.values() for cap in caps]\n",
    "    tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(all_captions)\n",
    "    return tokenizer\n",
    "\n",
    "# Encode and pad captions\n",
    "def encode_and_pad_captions(captions, tokenizer, max_length):\n",
    "    encoded_captions = {}\n",
    "    for image, caps in captions.items():\n",
    "        encoded_captions[image] = pad_sequences(\n",
    "            tokenizer.texts_to_sequences(caps), maxlen=max_length, padding=\"post\"\n",
    "        )\n",
    "    return encoded_captions\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading captions...\")\n",
    "    captions = load_captions(captions_file)\n",
    "\n",
    "    print(\"Cleaning captions...\")\n",
    "    cleaned_captions = preprocess_captions(captions)\n",
    "\n",
    "    print(\"Tokenizing captions...\")\n",
    "    tokenizer = tokenize_captions(cleaned_captions)\n",
    "    vocab_size = len(tokenizer.word_index) + 1  # Include padding token\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "    max_caption_length = max(len(cap.split()) for caps in cleaned_captions.values() for cap in caps)\n",
    "    print(f\"Maximum caption length: {max_caption_length}\")\n",
    "\n",
    "    print(\"Encoding and padding captions...\")\n",
    "    encoded_captions = encode_and_pad_captions(cleaned_captions, tokenizer, max_caption_length)\n",
    "\n",
    "    print(\"Saving tokenizer and processed captions...\")\n",
    "    with open(tokenizer_file, \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    np.save(processed_captions_file, encoded_captions, allow_pickle=True)\n",
    "\n",
    "    print(f\"Tokenizer saved to {tokenizer_file}\")\n",
    "    print(f\"Processed captions saved to {processed_captions_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Image: 1000268201_693b08cb0e.jpg\n",
      "Encoded Caption: [   2   41    3    2   89  169    6  118   52    2  394   11  391    3\n",
      "   27 5200  692    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n",
      "Decoded Caption: a child in a pink dress is climbing up a set of stairs in an entry way\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer_file = \"/Users/aryan/Desktop/Image-Caption-Generator/data/tokenizer.pkl\"\n",
    "with open(tokenizer_file, \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Load processed captions\n",
    "processed_captions_file = \"/Users/aryan/Desktop/Image-Caption-Generator/data/processed_captions.npy\"\n",
    "processed_captions = np.load(processed_captions_file, allow_pickle=True).item()\n",
    "\n",
    "# Sample Image and Caption\n",
    "sample_image = list(processed_captions.keys())[0]\n",
    "sample_encoded_caption = processed_captions[sample_image][0]\n",
    "\n",
    "# Decode the caption\n",
    "decoded_caption = \" \".join(\n",
    "    [tokenizer.index_word.get(index, \"<unk>\") for index in sample_encoded_caption if index > 0]\n",
    ")\n",
    "\n",
    "print(f\"Sample Image: {sample_image}\")\n",
    "print(f\"Encoded Caption: {sample_encoded_caption}\")\n",
    "print(f\"Decoded Caption: {decoded_caption}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 1000268201_693b08cb0e.jpg\n",
      "Decoded Caption: a child in a pink dress is climbing up a set of stairs in an entry way\n",
      "--------------------------------------------------\n",
      "Image: 1001773457_577c3a7d70.jpg\n",
      "Decoded Caption: a black dog and a spotted dog are fighting\n",
      "--------------------------------------------------\n",
      "Image: 1002674143_1b742ab4b8.jpg\n",
      "Decoded Caption: a little girl covered in paint sits in front of a painted rainbow with her hands in a bowl\n",
      "--------------------------------------------------\n",
      "Image: 1003163366_44323f5815.jpg\n",
      "Decoded Caption: a man lays on a bench while his dog sits by him\n",
      "--------------------------------------------------\n",
      "Image: 1007129816_e794419615.jpg\n",
      "Decoded Caption: a man in an orange hat starring at something\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Iterate through a few more images and their captions\n",
    "for i, image in enumerate(processed_captions.keys()):\n",
    "    if i == 5:  # Inspect the first 5 images\n",
    "        break\n",
    "    sample_encoded_caption = processed_captions[image][0]\n",
    "    decoded_caption = \" \".join(\n",
    "        [tokenizer.index_word.get(index, \"<unk>\") for index in sample_encoded_caption if index > 0]\n",
    "    )\n",
    "    print(f\"Image: {image}\")\n",
    "    print(f\"Decoded Caption: {decoded_caption}\")\n",
    "    print(\"-\" * 50) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
